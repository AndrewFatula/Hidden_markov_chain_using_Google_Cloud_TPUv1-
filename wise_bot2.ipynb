{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wise_bot2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUgO822TV8Vc6gmUIaCmul",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewFatula/Seq2seq_model_using_Google_Cloud_TPU/blob/master/wise_bot2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1LmNiXZGrOj",
        "colab_type": "code",
        "outputId": "cfd8ca41-0546-4d6f-ffc0-f7dc8ee004e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.translate import bleu_score\n",
        "from keras import regularizers\n",
        "from copy import deepcopy as dc\n",
        "import tensorboardcolab as tb\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITatQAIWGt2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade auth\n",
        "!pip uninstall grpcio\n",
        "!pip uninstall tensorflow\n",
        "!pip install grpcio==1.24.3\n",
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBc0KRiTeoMJ",
        "colab_type": "text"
      },
      "source": [
        "In order to use google cloud TPUv1 with distributed TPU strategy in eager execution mode we need to reinstall tensorflow 2.0.0 and grpcio 1.24.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-agojJLll8L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uSn1Ny_fGVj",
        "colab_type": "text"
      },
      "source": [
        "Authentication for transfering data from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "183hew7hG4VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'17XFYmocGDatGd40cAijhCOkI_zRXN5Mi'}) # all phrases in movies\n",
        "downloaded.GetContentFile('wise_quotes.txt') \n",
        "downloaded = drive.CreateFile({'id':'1Gn8fDg8Yhv8k0AN4XmKt8C9om_RgvyTu'}) # all phrases in movies\n",
        "downloaded.GetContentFile('wise_quotes2.txt') \n",
        "downloaded = drive.CreateFile({'id':'1cibzj_h-IDmoNCYcv9wiOjHFh34XhqzD'}) # all phrases in movies\n",
        "downloaded.GetContentFile('wise_quotes3.txt') \n",
        "downloaded = drive.CreateFile({'id':'1r5SQab9h4f-nyXu8rt-et0iiGdSEJ6rm'}) # all phrases in movies\n",
        "downloaded.GetContentFile('quotes.csv') \n",
        "downloaded = drive.CreateFile({'id':'1yNvbktnsPaqG7OkEfhyxV-yrI2TiP-PD'}) # all phrases in movies\n",
        "downloaded.GetContentFile('bible_quotes.csv') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY-1lxB5fMKL",
        "colab_type": "text"
      },
      "source": [
        "Transering needed data from Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR2_aBefG9X-",
        "colab_type": "code",
        "outputId": "28d22c09-950f-4d06-b739-e6998947a838",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address iis', tpu_address)\n",
        "\n",
        "\n",
        "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
        "    tpu=tpu_address)\n",
        "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
        "\n",
        "print ('Number of devices: {}'.format(tpu_strategy.num_replicas_in_sync))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address iis grpc://10.18.84.2:8470\n",
            "INFO:tensorflow:Initializing the TPU system: 10.18.84.2:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.18.84.2:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of devices: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUiEiXodfTAV",
        "colab_type": "text"
      },
      "source": [
        "Initializing available TPU devices and defining TPU_Distributed_strategy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50N32vjplsOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_QUOTE_LENGTH = 30\n",
        "MIN_QUOTE_LENGTH = 5\n",
        "MIN_TOKEN_FREQ = 5\n",
        "QUOTES_FILES = [\"wise_quotes.txt\", \"wise_quotes2.txt\", \"wise_quotes3.txt\", \"quotes.csv\", \"bible_quotes.csv\"]\n",
        "\n",
        "\n",
        "def read_quotes(quotes_files):\n",
        "\n",
        "    \"\"\" Function that reads all the data  (philosophical quotes from txt files, \n",
        "        bible quotes from csv file and philosophical quotes from csv file)\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Reading quotes...\")\n",
        "\n",
        "    lengths = []\n",
        "    tokenized_lines = []\n",
        "    freq_count = Counter()\n",
        "    tokenizer = TweetTokenizer()\n",
        "\n",
        "    #reading data from csv file\n",
        "    additional_quotes = list(pd.read_csv(quotes_files[3], sep = \",\")[\"quote\"])\n",
        "    bible_quotes = list(pd.read_csv(quotes_files[4], sep = \",\")[\"english\"])\n",
        "\n",
        "    #reading data from txt files\n",
        "    for quotes_file in quotes_files[:3]:\n",
        "        with open(quotes_file, \"r\") as quotes_lines:\n",
        "            for line in quotes_lines:\n",
        "                line = line.lower()\n",
        "                tokenized = tokenizer.tokenize(line)\n",
        "                if len(tokenized) < MAX_QUOTE_LENGTH + 1 and len(tokenized) > MIN_TOKEN_FREQ:\n",
        "                    freq_count.update(tokenized)\n",
        "                    tokenized_lines.append(tokenized)\n",
        "\n",
        "    #tokenization of all readed string lines\n",
        "    for line in additional_quotes + bible_quotes:\n",
        "        line = line.lower()\n",
        "        line = line.strip('\"')\n",
        "        line = line.strip(\"'\")\n",
        "        tokenized = tokenizer.tokenize(line)\n",
        "        if len(tokenized) < MAX_QUOTE_LENGTH + 1 and len(tokenized) > MIN_TOKEN_FREQ:\n",
        "            freq_count.update(tokenized)\n",
        "            tokenized_lines.append(tokenized)\n",
        "\n",
        "    #construction of words dictionary\n",
        "    word_set = list(map( lambda x: '+' + x[0] if x[1] >= MIN_TOKEN_FREQ else '-' + x[0], freq_count.items() ))\n",
        "    word_dict = {\"EMPTY\":0, \"BEGIN\":1, \"END\" : 2}\n",
        "    i = 3\n",
        "    for word in word_set:\n",
        "        if word[0] == \"+\":\n",
        "            word_dict[word[1:]] = i\n",
        "            i+=1\n",
        "\n",
        "    #filtering all tokenized quotes (leaving ones with known words)\n",
        "    quotes_tokens = []\n",
        "    for quote in tokenized_lines:\n",
        "        tokens = []\n",
        "        for word in quote:\n",
        "            if word in word_dict.keys():\n",
        "                tokens.append(word_dict[word])\n",
        "            else:\n",
        "                tokens.append(word_dict[\"EMPTY\"])\n",
        "        if not word_dict[\"EMPTY\"] in tokens:\n",
        "            tokens = [word_dict[\"BEGIN\"]] + tokens + [word_dict[\"END\"]]\n",
        "            quotes_tokens.append(tokens)\n",
        "            lengths.append(len(tokens))\n",
        "\n",
        "    print(\"Quotes are read and tokenized...\")\n",
        "    return quotes_tokens, word_dict, lengths\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp18SFEqluG3",
        "colab_type": "code",
        "outputId": "d638be9b-150f-4bc2-8b8a-e33b211bac85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "tokenized_lines, word_dict, true_lengths = read_quotes(QUOTES_FILES)\n",
        "\n",
        "quotes_width = int(max(true_lengths))\n",
        "sparse_quotes = np.zeros((len(true_lengths), quotes_width))\n",
        "quotes_length = len(true_lengths) \n",
        "\n",
        "inverse_word_dict = {}\n",
        "for key in word_dict.keys():\n",
        "    inverse_word_dict[word_dict[key]] = key\n",
        "\n",
        "#padding each sequence in x_data and y_data\n",
        "for i in range(quotes_length):\n",
        "    sparse_quotes[i, : true_lengths[i]] = np.array(tokenized_lines[i])\n",
        "\n",
        "unique_tokens = np.unique(sparse_quotes)\n",
        "\n",
        "transfer_tokens = {}\n",
        "transfer_dict = {}\n",
        "for i in range(len(unique_tokens)):\n",
        "    transfer_tokens[unique_tokens[i]] = i\n",
        "    transfer_dict[inverse_word_dict[unique_tokens[i]]] = i\n",
        "\n",
        "inverse_word_dict = {}\n",
        "\n",
        "for key in transfer_dict.keys():\n",
        "    inverse_word_dict[transfer_dict[key]] = key\n",
        "\n",
        "word_dict = transfer_dict\n",
        "\n",
        "for i in range(quotes_length):\n",
        "    for j in range(quotes_width):\n",
        "        sparse_quotes[i,j] = transfer_tokens[sparse_quotes[i,j]]\n",
        "\n",
        "\n",
        "true_lengths = tf.convert_to_tensor(true_lengths, dtype = tf.float32)\n",
        "sparse_quotes = tf.convert_to_tensor(sparse_quotes, dtype = tf.float32)\n",
        "\n",
        "print(\"Dictionary_size: \", len(word_dict))\n",
        "print(\"N of tokenized_quotes:\", len(tokenized_lines))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading quotes...\n",
            "Quotes are read and tokenized...\n",
            "Dictionary_size:  22134\n",
            "N of tokenized_quotes: 256487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NinKyn7mciaN",
        "colab_type": "text"
      },
      "source": [
        "Code above padds all word tokens and converts them to tensorflow arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAUhWd7SlwBZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "\n",
        "\n",
        "    class Phrase_model(tf.keras.Model):\n",
        "\n",
        "        ''' inherited from tf.keras.model class simle seq2seq model class with one embedding layer, 2 recurrent layers and two dense layers  '''\n",
        "\n",
        "        def __init__( self, hidden_size, emb_size, random_size, quote_len,\n",
        "                      word_dict, inverse_word_dict ):\n",
        "\n",
        "            super(Phrase_model, self).__init__()\n",
        "            self.dict_size = len(word_dict)\n",
        "            self.hidden_size = hidden_size\n",
        "            self.emb_size = emb_size\n",
        "            self.quote_len = quote_len\n",
        "            self.word_dict = word_dict\n",
        "            self.inverse_word_dict = inverse_word_dict\n",
        "\n",
        "            self.emb_layer = tf.keras.layers.Embedding(self.dict_size, emb_size)\n",
        "    \n",
        "            self.generator = tf.keras.layers.LSTM( units = hidden_size, recurrent_activation = \"sigmoid\",\n",
        "                                               kernel_regularizer=regularizers.l2(0.003), \n",
        "                                               recurrent_regularizer=regularizers.l2(0.003),\n",
        "                                               bias_regularizer=regularizers.l2(0.003),\n",
        "                                               dropout = 0.05, recurrent_dropout = 0.01, \n",
        "                                               return_state = True, return_sequences = True, unroll = True )\n",
        "            \n",
        "            self.interpreter = tf.keras.layers.Dense(self.dict_size)\n",
        "\n",
        "            \n",
        "        def generate_sequence_teacher(self, true_sequences):\n",
        "\n",
        "            ''' method for training seq2seq with teacher \n",
        "                witch takes as argument encoded hidden state from x_input sequence\n",
        "            '''\n",
        "            emb_sequences = self.emb_layer(true_sequences)\n",
        "\n",
        "            output, _, _ = self.generator(emb_sequences)\n",
        "            #output, _ = self.generator(emb_sequences)\n",
        "            return self.interpreter(output)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyN25JUsc6_k",
        "colab_type": "text"
      },
      "source": [
        "In code above you can see Pharse_model class used for training Hidden Markov Chain, subclassed from tf.keras.Model class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8KMGs5ElydZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "\n",
        "    def compute_loss( labels, generated_output, semihot_sequences ):\n",
        "\n",
        "        ''' cross_entropy_loss function for distributed strategy'''\n",
        "\n",
        "        per_example_loss = loss_object(labels, generated_output) * semihot_sequences\n",
        "        return tf.nn.compute_average_loss(per_example_loss , global_batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "    @tf.function\n",
        "    def teacher_train_step(inputs):\n",
        "\n",
        "        ''' train_step1 fuction for crossentropy teacher training method \n",
        "            of training LSTM cell, that will be crucial in construction Hidden Markov Chain\n",
        "        '''\n",
        "\n",
        "        true_sequences = inputs[0]\n",
        "        true_semihot = inputs[1]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            generated_output = seq2seq.generate_sequence_teacher(true_sequences[:, :-1])\n",
        "            loss = compute_loss(true_sequences[:, 1:], generated_output, true_semihot[:, 1:])\n",
        "        gradients = tape.gradient(loss, seq2seq.trainable_variables)\n",
        "        optimizer1.apply_gradients(zip(gradients, seq2seq.trainable_variables))\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "    def distributed_teacher_train_step(dataset_inputs):\n",
        "\n",
        "        ''' function that applies distriuted TPU strategy to train_step2 function'''\n",
        "\n",
        "        per_replica_losses = tpu_strategy.experimental_run_v2(teacher_train_step,\n",
        "                                                            args=(dataset_inputs,))\n",
        "        return tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                                                    axis=None)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjOy-5WoddVb",
        "colab_type": "text"
      },
      "source": [
        "In code above are defined distributed_train_step function decorated with @tf.fucntion for construction static computation graph in order to gain all benefits of using Google cloud TPU V1 and distrivuted tpu_strategy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3UpJ-pRl07g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "\n",
        "    def main_training_loop(train_dist_dataset, teacher_ptob, crossentropy_losses):\n",
        "\n",
        "        ''' training function for crossentropy training loop\n",
        "        '''\n",
        "        for x in train_dist_dataset:\n",
        "            loss = distributed_teacher_train_step(x)\n",
        "            crossentropy_losses.append(loss)\n",
        "\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdjsO86qd9gv",
        "colab_type": "text"
      },
      "source": [
        "Above is defined function to perform training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXOZTlkll3Xg",
        "colab_type": "code",
        "outputId": "311c3b13-5585-4b86-c783-149fe7d0145c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EMBEDDING_SIZE = 128\n",
        "RANDOM_SIZE = 128\n",
        "HIDDEN_SIZE = 2048\n",
        "BATCH_SIZE = 8192\n",
        "\n",
        "N_EPOCHS = 100\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(1)\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "\n",
        "\tdict_size = len(word_dict)\n",
        "\tseq2seq = Phrase_model( HIDDEN_SIZE, EMBEDDING_SIZE, RANDOM_SIZE, MAX_QUOTE_LENGTH + 2,\n",
        "\t                word_dict, inverse_word_dict )\n",
        "\n",
        "\n",
        "\ttrain_length = np.shape(sparse_quotes)[0]\n",
        "\ttrain_steps = int(train_length/BATCH_SIZE)\n",
        "\n",
        "\tmean_crossentropy_losses = []\n",
        "\tmean_discriminator_losses = []\n",
        "\tmean_generator_losses = []\n",
        "\n",
        "\tLEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(0.001, decay_steps = train_steps, decay_rate = 0.95, staircase= True)  \n",
        "\tloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\toptimizer1 = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\n",
        "\n",
        "\n",
        "\tprint(\"start training ... \")\n",
        "\tfor epoch in range(N_EPOCHS):\n",
        "\t\tepoch_start = time.localtime(time.time())\n",
        "\n",
        "\t\t\n",
        "\t\tcrossentropy_losses = []\n",
        "\n",
        "\t\ttrue_semihot = tf.ones((train_length, MAX_QUOTE_LENGTH + 2), dtype = tf.float32) * tf.range(1, MAX_QUOTE_LENGTH + 3, dtype = tf.float32)[None, :]\n",
        "\t\ttrue_semihot = tf.where(true_semihot <= true_lengths[ :, None], 1., 0.)\n",
        "\t\ttrue_semihot = tf.cast(true_semihot, tf.float32)\n",
        "\n",
        "\t\tdataset = tf.data.Dataset.from_tensor_slices((sparse_quotes, true_semihot)).shuffle(150000).batch(BATCH_SIZE, drop_remainder = True) \n",
        "\t\ttrain_dist_dataset = tpu_strategy.experimental_distribute_dataset(dataset)\n",
        "\n",
        "\t\tmain_training_loop(train_dist_dataset, teacher_prob, crossentropy_losses)\n",
        "\n",
        "\t\tepoch_end = time.localtime(time.time())\n",
        "\t\tstart_in_sec = epoch_start[3]*3600 + epoch_start[4]*60 + epoch_start[5]\n",
        "\t\tend_in_sec = epoch_end[3]*3600 + epoch_end[4]*60 + epoch_end[5]\n",
        "\t\tepoch_time = end_in_sec - start_in_sec\n",
        "\n",
        "\n",
        "\t\tcrossentropy_loss = np.mean(crossentropy_losses)\n",
        "\t\tmean_crossentropy_losses.append(crossentropy_loss)\n",
        "\n",
        "\t\tprint( \"Epoch: \", epoch, \", loss:\", crossentropy_loss, \"Teacher_prob: \", teacher_prob,\n",
        "\t\t\t\t\t\", time: \", epoch_time ,\" sec \"  )\n",
        "\n",
        "\t\t\n",
        "\n",
        "\n",
        "\n",
        "\t\t\n",
        "\n",
        "\n",
        "\t\t\t\t\n",
        "\t\t\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start training ... \n",
            "Epoch:  0 , loss: 15.786491 Teacher_prob:  1 , time:  38  sec \n",
            "Epoch:  1 , loss: 15.142873 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  2 , loss: 15.01017 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  3 , loss: 14.937063 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  4 , loss: 14.883511 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  5 , loss: 14.841157 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  6 , loss: 14.804003 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  7 , loss: 14.77085 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  8 , loss: 14.73738 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  9 , loss: 14.707952 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  10 , loss: 14.680026 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  11 , loss: 14.655442 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  12 , loss: 14.629939 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  13 , loss: 14.608069 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  14 , loss: 14.586387 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  15 , loss: 14.566985 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  16 , loss: 14.548858 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  17 , loss: 14.530775 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  18 , loss: 14.515153 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  19 , loss: 14.498131 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  20 , loss: 14.483783 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  21 , loss: 14.469545 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  22 , loss: 14.45643 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  23 , loss: 14.444648 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  24 , loss: 14.43218 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  25 , loss: 14.421263 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  26 , loss: 14.411096 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  27 , loss: 14.400289 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  28 , loss: 14.389426 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  29 , loss: 14.38119 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  30 , loss: 14.372435 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  31 , loss: 14.362799 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  32 , loss: 14.356068 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  33 , loss: 14.347804 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  34 , loss: 14.339562 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  35 , loss: 14.332039 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  36 , loss: 14.326163 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  37 , loss: 14.3188505 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  38 , loss: 14.311999 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  39 , loss: 14.306617 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  40 , loss: 14.301487 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  41 , loss: 14.295931 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  42 , loss: 14.291011 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  43 , loss: 14.286053 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  44 , loss: 14.281193 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  45 , loss: 14.276401 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  46 , loss: 14.271893 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  47 , loss: 14.268782 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  48 , loss: 14.263909 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  49 , loss: 14.260121 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  50 , loss: 14.2569275 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  51 , loss: 14.253389 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  52 , loss: 14.249349 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  53 , loss: 14.247286 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  54 , loss: 14.244251 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  55 , loss: 14.240566 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  56 , loss: 14.237828 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  57 , loss: 14.235148 Teacher_prob:  1 , time:  25  sec \n",
            "Epoch:  58 , loss: 14.232218 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  59 , loss: 14.2304125 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  60 , loss: 14.227309 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  61 , loss: 14.225448 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  62 , loss: 14.224213 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  63 , loss: 14.22183 Teacher_prob:  1 , time:  22  sec \n",
            "Epoch:  64 , loss: 14.220086 Teacher_prob:  1 , time:  23  sec \n",
            "Epoch:  65 , loss: 14.218741 Teacher_prob:  1 , time:  23  sec \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-705b3627c32b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_main_training_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                         \u001b[0mmain_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrossentropy_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-69-627dbad8f2d4>\u001b[0m in \u001b[0;36mmain_training_loop\u001b[0;34m(train_dist_dataset, teacher_ptob, generator_losses)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistributed_teacher_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistributed_chain_train_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-68-f8ec247b781e>\u001b[0m in \u001b[0;36mdistributed_teacher_train_step\u001b[0;34m(dataset_inputs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         per_replica_losses = tpu_strategy.experimental_run_v2(teacher_train_step,\n\u001b[0;32m--> 125\u001b[0;31m                                                             args=(dataset_inputs,))\n\u001b[0m\u001b[1;32m    126\u001b[0m         return tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n\u001b[1;32m    127\u001b[0m                                                     axis=None)\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py\u001b[0m in \u001b[0;36mexperimental_run_v2\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# so autograph is on by default here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/tpu_strategy.py\u001b[0m in \u001b[0;36mtpu_run\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtpu_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tpu_function_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 617\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_tpu_function_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    492\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEyg19GDeEyv",
        "colab_type": "text"
      },
      "source": [
        "Above is defined main training Cycle for training Hidden Markov Chain on philosophical and bible quotes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NQxpiTPxX6C",
        "colab_type": "code",
        "outputId": "da67b78d-c783-4116-a424-91abb100be6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.device(\"cpu\"):\n",
        "\n",
        "    \n",
        "    generated_sequence, _ = seq2seq.generate_sequence(30,1)\n",
        "\n",
        "    generated_sequence = generated_sequence.numpy()\n",
        "\n",
        "    for idx in range(30):\n",
        "        print(\"\\n\")\n",
        "        text_recreated = \"\"\n",
        "        for token in generated_sequence[idx]:\n",
        "            if token == 2:\n",
        "                break\n",
        "            text_recreated += inverse_word_dict[token] + \" \"\n",
        "\n",
        "        print(text_recreated)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "he who has a why to live can bear almost any how . \n",
            "\n",
            "\n",
            "we are all born for love . it is the principle of existence , and its only end . \n",
            "\n",
            "\n",
            "it's not the end of the road , it's just the end of the road work . \n",
            "\n",
            "\n",
            "what is the point of having a civilization , if we do not practice being civilized ! \n",
            "\n",
            "\n",
            "i am a great believer in luck , and i find the harder i work the more i have of it . \n",
            "\n",
            "\n",
            "the only thing that makes life possible is permanent , intolerable uncertainty : not knowing what comes next . \n",
            "\n",
            "\n",
            "we are all born for love . it is the principle of existence , and its only end . \n",
            "\n",
            "\n",
            "every man is a damn fool for at least five minutes every day wisdom consists in not exceeding the limit . \n",
            "\n",
            "\n",
            "pain is a part of life . i hated that fact . i would be much happier without it but then it wouldn't be my life . \n",
            "\n",
            "\n",
            "everybody has a dream , to make that dream a reality , one must work hard . \n",
            "\n",
            "\n",
            "eternity is a long time to spend alone , without others of your kind . \n",
            "\n",
            "\n",
            "it's not the end of the road , it's just the end of the road work . \n",
            "\n",
            "\n",
            "that is the difference between good teachers and great teachers : good teachers make the best of a pupil's means great teachers foresee a pupil's ends . \n",
            "\n",
            "\n",
            "every man is a damn fool for at least five minutes every day wisdom consists in not exceeding the limit . \n",
            "\n",
            "\n",
            "to be a success , follow your obsessions obsessively . \n",
            "\n",
            "\n",
            "the only thing that makes life possible is permanent , intolerable uncertainty : not knowing what comes next . \n",
            "\n",
            "\n",
            "a man who is not afraid is not aggressive , a man who has no sense of fear of any kind is really a free , a peaceful man . \n",
            "\n",
            "\n",
            "leaders are visionaries with a poorly developed sense of fear and no concept of the odds against them . they make the impossible happen . \n",
            "\n",
            "\n",
            "his eyes are so clear and blue that nothing but clichs enter my mind . \n",
            "\n",
            "\n",
            "i am a great believer in luck , and i find the harder i work the more i have of it . \n",
            "\n",
            "\n",
            "happiness is a choice . happiness does not depend on outward conditions but on inward decisions . \n",
            "\n",
            "\n",
            "no one can be a better you than you . therefore , striving continuously to become a better you is the only path to your greatness . \n",
            "\n",
            "\n",
            "i am a great believer in luck , and i find the harder i work the more i have of it . \n",
            "\n",
            "\n",
            "he who has a why to live can bear almost any how . \n",
            "\n",
            "\n",
            "heat roared within him . greedy heat . he wanted her ... she was in over her head , and it seemed ... so was he . \n",
            "\n",
            "\n",
            "i am a great believer in luck , and i find the harder i work the more i have of it . \n",
            "\n",
            "\n",
            "it is not the size of the ring , but the size of the love . \n",
            "\n",
            "\n",
            "the only thing that makes life possible is permanent , intolerable uncertainty : not knowing what comes next . \n",
            "\n",
            "\n",
            "a man who is not afraid is not aggressive , a man who has no sense of fear of any kind is really a free , a peaceful man . \n",
            "\n",
            "\n",
            "i am a great believer in luck , and i find the harder i work the more i have of it . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-Xqb77YhWBe",
        "colab_type": "code",
        "outputId": "ee3301ed-7ddd-49a4-b146-470d783db8a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with tf.device(\"cpu\"):\n",
        "\n",
        "    \n",
        "    generated_sequence, _ = seq2seq.generate_sequence(30,7)\n",
        "\n",
        "    generated_sequence = generated_sequence.numpy()\n",
        "\n",
        "    for idx in range(30):\n",
        "        print(\"\\n\")\n",
        "        text_recreated = \"\"\n",
        "        for token in generated_sequence[idx]:\n",
        "            if token == 2:\n",
        "                break\n",
        "            text_recreated += inverse_word_dict[token] + \" \"\n",
        "\n",
        "        print(text_recreated)\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "i am a great believer in luck , and i find the harder i work the more i have of it . \n",
            "\n",
            "\n",
            "travel is the best teacher . the only way to an open mind is by taking a plane out into the open world . \n",
            "\n",
            "\n",
            "that was the thing about weddings : they forced family members to deal with one another , like it or not . \n",
            "\n",
            "\n",
            "you can  t get the taste of winning in a running competition with a turtle ! \n",
            "\n",
            "\n",
            "no one can be a better you than you . therefore , striving continuously to become a better you is the only path to your greatness . \n",
            "\n",
            "\n",
            "my dad was a big frank zappa fan , so i remember listening to a lot of frank zappa . girls do not like frank zappa . \n",
            "\n",
            "\n",
            "it is not the size of the ring , but the size of the love . \n",
            "\n",
            "\n",
            "sexy can really lead you in a small one . \n",
            "\n",
            "\n",
            "i'm not a big fan of talking about dying . and then i make a movie where i kill everybody . \n",
            "\n",
            "\n",
            "to be a success , follow your obsessions obsessively . \n",
            "\n",
            "\n",
            "if you want to be happy , be . \n",
            "\n",
            "\n",
            "my father was a man , and i know the sex pretty well . \n",
            "\n",
            "\n",
            "your life is a reflection of the quality of the questions you ask yourself and others . \n",
            "\n",
            "\n",
            "don't let the fear of failure keep you away from your destiny . \n",
            "\n",
            "\n",
            "and the king said unto him , why speakest thou any more of thy matters ? \n",
            "\n",
            "\n",
            "don't let the fear of failure keep you away from your destiny . \n",
            "\n",
            "\n",
            "ye shall not do after all the things that we do here this day , every man whatsoever is right in his own eyes ; \n",
            "\n",
            "\n",
            "never make a decision while in the state of anger . \n",
            "\n",
            "\n",
            "every man is a damn fool for at least five minutes every day wisdom consists in not exceeding the limit . \n",
            "\n",
            "\n",
            "those who are not shocked when they first come across quantum theory cannot possibly have understood it . \n",
            "\n",
            "\n",
            "everyone has a story to tell . everyone is a writer , some are written in the books and some are confined to hearts . \n",
            "\n",
            "\n",
            "it is not the size of the ring , but the size of the love . \n",
            "\n",
            "\n",
            "be a person who radiates the beauty of love to extinguish the fire of hatred . \n",
            "\n",
            "\n",
            "i have a very strong feeling that the opposite of love is not hate - it's apathy . it's not giving a damn . \n",
            "\n",
            "\n",
            "the only way to get the best out of the diminishing life is to invest it into the fulfilment of your purpose . \n",
            "\n",
            "\n",
            "the best way to remember your wife's birthday is to forget it once . \n",
            "\n",
            "\n",
            "it is not the size of the ring , but the size of the love . \n",
            "\n",
            "\n",
            "the only thing that makes life possible is permanent , intolerable uncertainty : not knowing what comes next . \n",
            "\n",
            "\n",
            "an artist is a dreamer consenting to dream of the actual world . \n",
            "\n",
            "\n",
            "it is not the size of the ring , but the size of the love . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql5lXrplVrjK",
        "colab_type": "code",
        "outputId": "2df98a88-f0f9-4636-acce-c211a79aca3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "phrases = [\n",
        "            \"it's better to have more\",\n",
        "            \"man\",\n",
        "            \"intelligence\",\n",
        "            \"sounds are like\",\n",
        "            \"strength\",\n",
        "            \"be or not to be\",\n",
        "            \"how to convince someone\", \n",
        "            \"set of words\", \n",
        "            \"not a friend\",\n",
        "            \"how to act\", \n",
        "            \"character\",\n",
        "            \"rude\",\n",
        "            \"seriousness\",\n",
        "            \"wise man\",\n",
        "            \"wise\"\n",
        "          ]\n",
        "\n",
        "def end_phrase(phrase):\n",
        "    phrase = [\"BEGIN\"] + phrase.split(\" \")\n",
        "    phrase_tokens = []\n",
        "    hidden_h = tf.zeros((1, HIDDEN_SIZE), dtype = tf.float32)\n",
        "    hidden_c = tf.zeros((1, HIDDEN_SIZE), dtype = tf.float32)\n",
        "\n",
        "    for word in phrase:\n",
        "        phrase_tokens.append(word_dict[word])\n",
        "\n",
        "    total_output = phrase_tokens[1:]\n",
        "\n",
        "    for token in phrase_tokens:\n",
        "        current_emb = seq2seq.emb_layer(tf.convert_to_tensor([[token]], dtype = tf.float32))\n",
        "        current_output, hidden_h, hidden_c = seq2seq.generator(current_emb, [hidden_h, hidden_c])\n",
        "        current_distribution = seq2seq.interpreter(current_output)\n",
        "\n",
        "    generated_token = tf.argmax(current_distribution, axis = -1)\n",
        "    total_output.append(generated_token.numpy()[0,0])\n",
        "    current_emb = seq2seq.emb_layer(generated_token)\n",
        "    \n",
        "    for _ in range(30):\n",
        "        current_output, hidden_h, hidden_c = seq2seq.generator(current_emb, [hidden_h, hidden_c])\n",
        "        current_distribution = seq2seq.interpreter(current_output)\n",
        "        generated_token  = tf.argmax(current_distribution, axis = -1)\n",
        "        current_emb = seq2seq.emb_layer(generated_token)\n",
        "        \n",
        "        if not ( generated_token.numpy()[0,0] == 2 or generated_token.numpy()[0,0] == 0):\n",
        "            total_output.append(generated_token.numpy()[0,0])\n",
        "        else:\n",
        "            break\n",
        "    \n",
        "    output_phrase = \"\"\n",
        "\n",
        "    for token in total_output:\n",
        "        output_phrase += inverse_word_dict[token] + \" \"\n",
        "\n",
        "    print(output_phrase)\n",
        "\n",
        "\n",
        "for phrase in phrases:\n",
        "    end_phrase(phrase)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it's better to have more life than offer and clear when i write about ' and ' will , not even a few can afford to be ! \n",
            "man is a social animal . \n",
            "intelligence is the ability to adapt to change . \n",
            "sounds are like going ' in the end . \n",
            "strength is not determined by the looks , but action . \n",
            "be or not to be rich and believe in your ability to write them out loud and do the rest of your dreams . \n",
            "how to convince someone else  s name and what is  matter with them ! \n",
            "set of words help but seek to manifest . \n",
            "not a friend goes by the justice industry . \n",
            "how to act : try to keep quiet . \n",
            "character is the ability to carry out a good resolution long after the excitement of the moment has passed . \n",
            "rude and wrong , it's right to be positive . \n",
            "seriousness is a series of which i even the young knows two thousand words , and that's now . \n",
            "wise man loses more than pleasure , for the best reason he is trying to make living choices . \n",
            "wise men speak because they have something to say \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT9dt2fM25yk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "beginnings = sparse_quotes.numpy()[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb4-iL1I_DX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "freq_count = Counter()\n",
        "freq_count.update(beginnings)\n",
        "freq_count = list(freq_count.items())\n",
        "freq_count.sort(key = lambda x: x[1], reverse = True)\n",
        "\n",
        "\n",
        "begin_tokens = [freq_count[i][0] for i in range(1500)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_z2ybm2Acwa",
        "colab_type": "code",
        "outputId": "f686f259-4d9a-4127-8e42-df37c7e82e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(begin_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[63.0, 77.0, 25.0, 140.0, 41.0, 61.0, 29.0, 66.0, 103.0, 67.0, 28.0, 70.0, 235.0, 11.0, 75.0, 39.0, 20.0, 217.0, 164.0, 369.0, 146.0, 170.0, 377.0, 166.0, 14.0, 46.0, 292.0, 206.0, 13.0, 350.0, 58.0, 86.0, 69.0, 256.0, 18.0, 366.0, 273.0, 74.0, 306.0, 261.0, 510.0, 351.0, 938.0, 55.0, 896.0, 173.0, 180.0, 159.0, 691.0, 254.0, 420.0, 129.0, 95.0, 309.0, 7.0, 104.0, 411.0, 222.0, 107.0, 59.0, 117.0, 1579.0, 536.0, 228.0, 333.0, 73.0, 1329.0, 82.0, 19.0, 27.0, 262.0, 472.0, 234.0, 33.0, 272.0, 44.0, 83.0, 1030.0, 944.0, 290.0, 153.0, 141.0, 348.0, 1120.0, 318.0, 35.0, 97.0, 831.0, 942.0, 280.0, 507.0, 681.0, 194.0, 365.0, 331.0, 464.0, 295.0, 868.0, 197.0, 249.0, 543.0, 573.0, 6.0, 393.0, 1187.0, 522.0, 608.0, 91.0, 144.0, 399.0, 438.0, 556.0, 71.0, 637.0, 470.0, 410.0, 102.0, 312.0, 390.0, 4756.0, 1590.0, 657.0, 5000.0, 1270.0, 345.0, 714.0, 1782.0, 2456.0, 4696.0, 1200.0, 469.0, 233.0, 2787.0, 236.0, 85.0, 100.0, 2159.0, 8.0, 532.0, 123.0, 424.0, 3023.0, 64.0, 751.0, 190.0, 22.0, 479.0, 40.0, 667.0, 352.0, 5091.0, 201.0, 722.0, 604.0, 1386.0, 1037.0, 264.0, 3708.0, 1393.0, 1462.0, 884.0, 283.0, 3119.0, 2765.0, 2978.0, 1361.0, 279.0, 37.0, 2900.0, 2451.0, 2350.0, 998.0, 185.0, 1173.0, 57.0, 1378.0, 145.0, 193.0, 34.0, 630.0, 520.0, 654.0, 918.0, 652.0, 9997.0, 126.0, 3174.0, 785.0, 169.0, 336.0, 829.0, 1466.0, 26.0, 291.0, 1072.0, 81.0, 693.0, 1338.0, 530.0, 2280.0, 583.0, 138.0, 1644.0, 4498.0, 911.0, 965.0, 1313.0, 260.0, 359.0, 1111.0, 324.0, 378.0, 87.0, 815.0, 434.0, 640.0, 1522.0, 3992.0, 1436.0, 1953.0, 397.0, 2956.0, 5205.0, 1705.0, 60.0, 679.0, 2302.0, 1876.0, 5492.0, 269.0, 224.0, 43.0, 631.0, 346.0, 1781.0, 577.0, 1390.0, 1259.0, 1271.0, 2802.0, 56.0, 2231.0, 1153.0, 1966.0, 2790.0, 12.0, 555.0, 1320.0, 961.0, 299.0, 76.0, 268.0, 1041.0, 1817.0, 243.0, 298.0, 2421.0, 2427.0, 892.0, 977.0, 3217.0, 747.0, 2322.0, 315.0, 443.0, 658.0, 932.0, 3301.0, 344.0, 3.0, 50.0, 101.0, 1507.0, 1561.0, 2106.0, 798.0, 574.0, 220.0, 1640.0, 822.0, 2930.0, 897.0, 21.0, 391.0, 468.0, 5659.0, 2912.0, 561.0, 2785.0, 3758.0, 2684.0, 703.0, 1568.0, 1841.0, 3704.0, 62.0, 2133.0, 3957.0, 4942.0, 94.0, 1464.0, 1610.0, 893.0, 2934.0, 5269.0, 2206.0, 567.0, 53.0, 540.0, 1412.0, 240.0, 1917.0, 833.0, 1211.0, 1289.0, 606.0, 386.0, 4104.0, 245.0, 2397.0, 800.0, 688.0, 360.0, 1062.0, 1456.0, 406.0, 1278.0, 980.0, 832.0, 1432.0, 3713.0, 724.0, 4015.0, 358.0, 3191.0, 5.0, 4561.0, 3501.0, 1014.0, 1834.0, 2755.0, 213.0, 821.0, 1894.0, 320.0, 2883.0, 1855.0, 2712.0, 957.0, 1318.0, 969.0, 4386.0, 413.0, 817.0, 131.0, 1942.0, 723.0, 441.0, 5140.0, 2941.0, 3908.0, 1732.0, 1354.0, 636.0, 1513.0, 3032.0, 755.0, 1294.0, 8031.0, 179.0, 3124.0, 186.0, 1328.0, 48.0, 4980.0, 5423.0, 168.0, 451.0, 432.0, 2829.0, 771.0, 1964.0, 1166.0, 296.0, 4794.0, 1805.0, 7599.0, 5665.0, 257.0, 2647.0, 99.0, 8102.0, 2368.0, 116.0, 92.0, 259.0, 2415.0, 471.0, 1483.0, 3661.0, 513.0, 287.0, 538.0, 3211.0, 1545.0, 5350.0, 2876.0, 2220.0, 480.0, 8849.0, 1081.0, 12631.0, 1963.0, 6261.0, 921.0, 459.0, 1147.0, 828.0, 882.0, 2570.0, 2929.0, 497.0, 529.0, 3185.0, 3865.0, 4367.0, 1660.0, 167.0, 3169.0, 2256.0, 370.0, 5029.0, 4977.0, 7303.0, 285.0, 3007.0, 203.0, 707.0, 1829.0, 550.0, 1101.0, 1058.0, 899.0, 4288.0, 776.0, 867.0, 389.0, 1198.0, 673.0, 2308.0, 1882.0, 9862.0, 2487.0, 289.0, 3805.0, 215.0, 994.0, 2743.0, 3266.0, 1168.0, 10205.0, 3464.0, 1548.0, 2512.0, 207.0, 232.0, 2548.0, 2050.0, 1482.0, 3215.0, 271.0, 3342.0, 174.0, 5587.0, 218.0, 1845.0, 239.0, 2470.0, 3356.0, 5150.0, 3016.0, 182.0, 5050.0, 3436.0, 774.0, 139.0, 1443.0, 1565.0, 319.0, 2998.0, 165.0, 10491.0, 4308.0, 4770.0, 599.0, 720.0, 31.0, 511.0, 1736.0, 7585.0, 405.0, 1514.0, 2705.0, 2168.0, 808.0, 2349.0, 4587.0, 311.0, 5855.0, 851.0, 1097.0, 1026.0, 1549.0, 118.0, 1117.0, 23.0, 2777.0, 1940.0, 5601.0, 3288.0, 3814.0, 1184.0, 1495.0, 3205.0, 9349.0, 4876.0, 2105.0, 1152.0, 4733.0, 2557.0, 1575.0, 325.0, 6124.0, 6437.0, 1254.0, 1016.0, 3792.0, 797.0, 617.0, 4365.0, 544.0, 4161.0, 14861.0, 1822.0, 5071.0, 304.0, 1922.0, 841.0, 363.0, 3413.0, 3831.0, 600.0, 1080.0, 2885.0, 7627.0, 6007.0, 3084.0, 1191.0, 313.0, 2720.0, 3765.0, 545.0, 900.0, 1325.0, 993.0, 2643.0, 4862.0, 566.0, 2006.0, 1406.0, 3002.0, 3085.0, 2240.0, 1919.0, 1791.0, 5179.0, 4122.0, 2952.0, 7754.0, 17.0, 6001.0, 1486.0, 534.0, 1275.0, 3025.0, 2078.0, 2924.0, 1218.0, 1454.0, 478.0, 1404.0, 684.0, 2300.0, 2063.0, 1255.0, 2527.0, 3471.0, 196.0, 898.0, 148.0, 1292.0, 3354.0, 1033.0, 1679.0, 3115.0, 307.0, 859.0, 877.0, 422.0, 926.0, 4497.0, 3579.0, 8856.0, 7315.0, 10140.0, 3137.0, 172.0, 147.0, 445.0, 1701.0, 1172.0, 3060.0, 924.0, 766.0, 3574.0, 2893.0, 7261.0, 1069.0, 541.0, 738.0, 995.0, 288.0, 1189.0, 1123.0, 5378.0, 162.0, 1183.0, 4132.0, 1699.0, 810.0, 15813.0, 3064.0, 498.0, 1205.0, 30.0, 3116.0, 2969.0, 248.0, 3901.0, 84.0, 500.0, 1496.0, 744.0, 322.0, 130.0, 908.0, 3875.0, 596.0, 2910.0, 1821.0, 1673.0, 1227.0, 8070.0, 463.0, 3088.0, 4757.0, 456.0, 834.0, 11158.0, 3549.0, 4329.0, 3914.0, 435.0, 24.0, 2867.0, 15429.0, 142.0, 1020.0, 916.0, 238.0, 1672.0, 1145.0, 3393.0, 402.0, 2406.0, 1068.0, 4773.0, 5413.0, 36.0, 701.0, 1656.0, 2103.0, 519.0, 2734.0, 183.0, 338.0, 1839.0, 1155.0, 1521.0, 1943.0, 2025.0, 642.0, 535.0, 626.0, 3618.0, 937.0, 4550.0, 2408.0, 3202.0, 5011.0, 3063.0, 6884.0, 1272.0, 8240.0, 6117.0, 375.0, 1396.0, 2526.0, 8550.0, 10203.0, 4521.0, 3415.0, 8903.0, 804.0, 143.0, 11302.0, 3020.0, 255.0, 643.0, 4927.0, 4507.0, 3092.0, 302.0, 1835.0, 601.0, 1731.0, 5294.0, 2296.0, 3204.0, 3021.0, 10187.0, 1844.0, 1896.0, 205.0, 3454.0, 861.0, 1724.0, 252.0, 4647.0, 122.0, 3079.0, 981.0, 2468.0, 1897.0, 2745.0, 198.0, 1526.0, 5429.0, 779.0, 1792.0, 3179.0, 5726.0, 3383.0, 3956.0, 3165.0, 2132.0, 1028.0, 1619.0, 1050.0, 933.0, 1635.0, 3643.0, 105.0, 2203.0, 850.0, 4420.0, 2976.0, 1064.0, 4814.0, 1581.0, 5426.0, 6737.0, 1828.0, 2682.0, 157.0, 570.0, 999.0, 265.0, 2072.0, 2245.0, 502.0, 1209.0, 3508.0, 715.0, 5485.0, 7982.0, 5127.0, 10555.0, 1410.0, 1413.0, 4100.0, 4103.0, 423.0, 2228.0, 6130.0, 2843.0, 1078.0, 2588.0, 680.0, 3289.0, 3307.0, 1811.0, 1787.0, 763.0, 761.0, 10233.0, 3816.0, 6116.0, 1091.0, 2026.0, 4953.0, 2762.0, 3098.0, 4861.0, 2894.0, 5438.0, 3507.0, 2895.0, 1681.0, 18370.0, 1573.0, 258.0, 293.0, 3277.0, 528.0, 1976.0, 1445.0, 6683.0, 246.0, 549.0, 1685.0, 4364.0, 1071.0, 3168.0, 5202.0, 4946.0, 909.0, 811.0, 3589.0, 4116.0, 10179.0, 4873.0, 6908.0, 200.0, 2966.0, 1418.0, 1440.0, 1647.0, 1290.0, 1633.0, 14109.0, 3706.0, 1426.0, 6813.0, 3477.0, 1617.0, 1056.0, 415.0, 781.0, 1577.0, 862.0, 3286.0, 3194.0, 4303.0, 1019.0, 4655.0, 2124.0, 404.0, 2372.0, 5704.0, 557.0, 3433.0, 175.0, 1913.0, 1883.0, 4658.0, 2903.0, 5576.0, 4570.0, 2227.0, 5341.0, 2567.0, 2152.0, 516.0, 1528.0, 5533.0, 4999.0, 353.0, 1340.0, 5233.0, 4970.0, 584.0, 4907.0, 5936.0, 2797.0, 9984.0, 14572.0, 10757.0, 7353.0, 1728.0, 6595.0, 4926.0, 2136.0, 1945.0, 230.0, 5014.0, 6663.0, 9428.0, 1331.0, 1646.0, 8009.0, 7070.0, 210.0, 8131.0, 4541.0, 4695.0, 3322.0, 671.0, 1696.0, 2791.0, 392.0, 2157.0, 3984.0, 4537.0, 4742.0, 687.0, 2798.0, 2045.0, 329.0, 7259.0, 7727.0, 4153.0, 9267.0, 3860.0, 1108.0, 10216.0, 1491.0, 474.0, 533.0, 6096.0, 161.0, 4845.0, 1176.0, 6145.0, 476.0, 2252.0, 2062.0, 1598.0, 7306.0, 4305.0, 4931.0, 6868.0, 18988.0, 4877.0, 721.0, 4033.0, 886.0, 8876.0, 4716.0, 6026.0, 6488.0, 843.0, 1040.0, 1960.0, 2379.0, 2959.0, 2719.0, 199.0, 2311.0, 4711.0, 4624.0, 361.0, 746.0, 2075.0, 5888.0, 2224.0, 93.0, 1221.0, 704.0, 4008.0, 2251.0, 1717.0, 7370.0, 955.0, 3252.0, 1073.0, 3808.0, 610.0, 2993.0, 1809.0, 1281.0, 1652.0, 1025.0, 1572.0, 8410.0, 1580.0, 16.0, 488.0, 274.0, 9.0, 4780.0, 2892.0, 3431.0, 2277.0, 2172.0, 4276.0, 14113.0, 1367.0, 1628.0, 700.0, 395.0, 2614.0, 2657.0, 1589.0, 1733.0, 416.0, 1547.0, 3189.0, 6330.0, 14943.0, 8156.0, 11344.0, 3274.0, 2825.0, 1816.0, 10227.0, 13022.0, 7037.0, 155.0, 569.0, 1219.0, 17870.0, 1137.0, 2430.0, 3245.0, 111.0, 477.0, 4385.0, 1994.0, 2514.0, 1162.0, 1710.0, 2994.0, 2524.0, 4614.0, 665.0, 6452.0, 1704.0, 1051.0, 3180.0, 634.0, 7747.0, 3782.0, 1727.0, 8433.0, 1296.0, 2985.0, 7792.0, 1517.0, 1438.0, 3836.0, 802.0, 11316.0, 8035.0, 5034.0, 1365.0, 501.0, 2234.0, 1509.0, 1305.0, 8567.0, 2793.0, 3251.0, 13217.0, 1530.0, 7341.0, 1691.0, 4671.0, 1578.0, 1261.0, 10704.0, 706.0, 8954.0, 418.0, 901.0, 7975.0, 1541.0, 5432.0, 741.0, 737.0, 3131.0, 1450.0, 4677.0, 1655.0, 4362.0, 689.0, 3973.0, 5973.0, 2575.0, 827.0, 1063.0, 3759.0, 1779.0, 1235.0, 3852.0, 7016.0, 1849.0, 9855.0, 2275.0, 9283.0, 879.0, 89.0, 1533.0, 3212.0, 2896.0, 5812.0, 1986.0, 305.0, 10603.0, 3696.0, 4815.0, 5211.0, 14704.0, 9986.0, 6858.0, 442.0, 2764.0, 904.0, 1130.0, 3237.0, 769.0, 300.0, 941.0, 4376.0, 1415.0, 4388.0, 5636.0, 1737.0, 467.0, 5286.0, 627.0, 12347.0, 11129.0, 4632.0, 1859.0, 1750.0, 5338.0, 2279.0, 9048.0, 4803.0, 9702.0, 47.0, 2980.0, 453.0, 5825.0, 1648.0, 1693.0, 2694.0, 855.0, 2139.0, 2918.0, 80.0, 3364.0, 4371.0, 3486.0, 4805.0, 1334.0, 3450.0, 3485.0, 3640.0, 1550.0, 4040.0, 2871.0, 6489.0, 1742.0, 5489.0, 7463.0, 2701.0, 414.0, 158.0, 6504.0, 3468.0, 1082.0, 2390.0, 2610.0, 813.0, 4306.0, 5075.0, 2150.0, 4097.0, 5514.0, 339.0, 3077.0, 1036.0, 2741.0, 5674.0, 10173.0, 8315.0, 11558.0, 13845.0, 1360.0, 6034.0, 1596.0, 2964.0, 5342.0, 7509.0, 4253.0, 3439.0, 6659.0, 7516.0, 4006.0, 1700.0, 4446.0, 3164.0, 5382.0, 2858.0, 5852.0, 6241.0, 2057.0, 3099.0, 971.0, 13256.0, 1671.0, 12254.0, 1394.0, 1571.0, 7894.0, 3537.0, 943.0, 9654.0, 3854.0, 372.0, 5845.0, 7206.0, 6422.0, 3076.0, 6418.0, 5688.0, 2832.0, 1230.0, 5844.0, 4847.0, 593.0, 13392.0, 13201.0, 1980.0, 3170.0, 403.0, 7134.0, 2017.0, 3774.0, 5985.0, 2319.0, 15116.0, 3414.0, 3497.0, 4542.0, 10763.0, 16296.0, 1239.0, 2604.0, 2807.0, 3801.0, 3081.0, 3056.0, 1358.0, 1385.0, 3591.0, 4594.0, 1778.0, 2925.0, 525.0, 1134.0, 2846.0, 5530.0, 125.0, 580.0, 6754.0, 3670.0, 857.0, 1208.0, 7431.0, 8062.0, 6512.0, 3732.0, 4682.0, 151.0, 4921.0, 3411.0, 2495.0, 9644.0, 2645.0, 9078.0, 8511.0, 5687.0, 10446.0, 10479.0, 7667.0, 537.0, 1907.0, 10843.0, 3695.0, 4391.0, 940.0, 890.0, 6182.0, 4898.0, 1416.0, 2822.0, 2556.0, 6991.0, 421.0, 2021.0, 3605.0, 475.0, 11730.0, 2612.0, 9939.0, 8154.0, 3494.0, 2747.0, 136.0, 3869.0, 587.0, 8669.0, 13548.0, 946.0, 3273.0, 2313.0, 4197.0, 4480.0, 3422.0, 10766.0, 394.0, 3166.0, 3068.0, 5380.0, 2257.0, 2210.0, 1346.0, 1525.0, 3909.0, 662.0, 242.0, 1010.0, 10785.0, 297.0, 10328.0, 6154.0, 5160.0, 13521.0, 3013.0, 4567.0, 3683.0, 5197.0, 6027.0, 1314.0, 14888.0, 571.0, 367.0, 13727.0, 10260.0, 8936.0, 6153.0, 9083.0, 1350.0, 1854.0, 15798.0, 328.0, 3080.0, 14866.0, 3646.0, 1924.0, 1160.0, 2212.0, 11576.0, 2796.0, 881.0, 10820.0, 1903.0, 3161.0, 7928.0, 13940.0, 3097.0, 2792.0, 14483.0, 3554.0, 1664.0, 3880.0, 3093.0, 9259.0, 4846.0, 17234.0, 6084.0, 14308.0, 16925.0, 382.0, 276.0, 2382.0, 1102.0, 3359.0, 3676.0, 973.0, 2651.0, 4829.0, 2116.0, 4690.0, 4775.0, 5255.0, 5344.0, 2060.0, 5891.0, 6310.0, 6541.0, 2630.0, 6788.0, 2472.0, 1501.0, 6123.0, 3406.0, 8061.0, 905.0, 8161.0, 2091.0, 160.0, 3172.0, 5146.0, 8551.0, 3008.0, 8652.0, 8907.0, 1797.0, 2440.0, 3603.0, 10014.0, 2137.0, 956.0, 805.0, 3981.0, 10546.0, 10580.0, 3330.0, 1465.0, 8993.0, 1217.0, 2416.0, 3866.0, 2286.0, 639.0, 799.0, 5653.0, 3586.0, 3049.0, 11282.0, 446.0, 1088.0, 5847.0, 9365.0, 1046.0, 4409.0, 542.0, 7389.0, 3006.0, 2860.0, 651.0, 5623.0, 910.0, 6323.0, 3577.0, 3046.0, 13987.0, 13997.0, 8381.0, 3120.0, 2960.0, 3199.0, 9580.0, 2951.0, 8106.0, 10103.0, 3318.0, 3768.0, 9272.0, 1045.0, 10056.0, 7870.0, 13692.0, 4117.0, 2926.0, 2167.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUdRJpVqtU2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import h5py\n",
        "import pickle\n",
        "\n",
        "weights = []\n",
        "weights.append(seq2seq.emb_layer.get_weights())\n",
        "weights.append(seq2seq.generator.get_weights())\n",
        "weights.append(seq2seq.interpreter.get_weights())\n",
        "\n",
        "def download_weights(weights):\n",
        "\n",
        "    weight_file_name = \"weights_wisebot_final3.\" +  \\\n",
        "                        str(MIN_QUOTE_LENGTH) + \".\" + \\\n",
        "                         str(MAX_QUOTE_LENGTH) + \".\" + \\\n",
        "                         str(MIN_TOKEN_FREQ) + \".h5\"\n",
        "\n",
        "    with h5py.File(weight_file_name, \"w\") as h5f:\n",
        "        i = 0\n",
        "        for weight in weights:\n",
        "            group = h5f.create_group(str(i))\n",
        "            i += 1\n",
        "            j = 0\n",
        "            for kernel in weight:\n",
        "                group.create_dataset(str(j), data=kernel)\n",
        "                j += 1\n",
        "                \n",
        "    h5f.close()\n",
        "    files.download(weight_file_name)\n",
        "\n",
        "\n",
        "\n",
        "words_file_name = \"words_wisebot_final3.\" +  \\\n",
        "                        str(MIN_QUOTE_LENGTH) + \".\" + \\\n",
        "                         str(MAX_QUOTE_LENGTH) + \".\" + \\\n",
        "                         str(MIN_TOKEN_FREQ) + \".dict\"\n",
        "\n",
        "with open(words_file_name, \"wb\") as dict_file:\n",
        "    pickle.dump(word_dict, dict_file)\n",
        "\n",
        "\n",
        "begin_tokens_file_name = \"begin_tokens.\" + str(MIN_QUOTE_LENGTH) + \".\" + \\\n",
        "                         str(MAX_QUOTE_LENGTH) + \".\" + \\\n",
        "                         str(MIN_TOKEN_FREQ) + \".arr\"\n",
        "\n",
        "with open(begin_tokens_file_name, \"wb\") as arr_file:\n",
        "    pickle.dump(begin_tokens, arr_file)\n",
        "\n",
        "download_weights(weights)\n",
        "files.download(begin_tokens_file_name)\n",
        "files.download(words_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvkwWdSyAiIm",
        "colab_type": "code",
        "outputId": "a89b7636-a66b-4719-c69c-f640c6350d95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(begin_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[63.0, 77.0, 25.0, 140.0, 41.0, 61.0, 29.0, 66.0, 103.0, 67.0, 28.0, 70.0, 235.0, 11.0, 75.0, 39.0, 20.0, 217.0, 164.0, 369.0, 146.0, 170.0, 377.0, 166.0, 14.0, 46.0, 292.0, 206.0, 13.0, 350.0, 58.0, 86.0, 69.0, 256.0, 18.0, 366.0, 273.0, 74.0, 306.0, 261.0, 510.0, 351.0, 938.0, 55.0, 896.0, 173.0, 180.0, 159.0, 691.0, 254.0, 420.0, 129.0, 95.0, 309.0, 7.0, 104.0, 411.0, 222.0, 107.0, 59.0, 117.0, 1579.0, 536.0, 228.0, 333.0, 73.0, 1329.0, 82.0, 19.0, 27.0, 262.0, 472.0, 234.0, 33.0, 272.0, 44.0, 83.0, 1030.0, 944.0, 290.0, 153.0, 141.0, 348.0, 1120.0, 318.0, 35.0, 97.0, 831.0, 942.0, 280.0, 507.0, 681.0, 194.0, 365.0, 331.0, 464.0, 295.0, 868.0, 197.0, 249.0, 543.0, 573.0, 6.0, 393.0, 1187.0, 522.0, 608.0, 91.0, 144.0, 399.0, 438.0, 556.0, 71.0, 637.0, 470.0, 410.0, 102.0, 312.0, 390.0, 4756.0, 1590.0, 657.0, 5000.0, 1270.0, 345.0, 714.0, 1782.0, 2456.0, 4696.0, 1200.0, 469.0, 233.0, 2787.0, 236.0, 85.0, 100.0, 2159.0, 8.0, 532.0, 123.0, 424.0, 3023.0, 64.0, 751.0, 190.0, 22.0, 479.0, 40.0, 667.0, 352.0, 5091.0, 201.0, 722.0, 604.0, 1386.0, 1037.0, 264.0, 3708.0, 1393.0, 1462.0, 884.0, 283.0, 3119.0, 2765.0, 2978.0, 1361.0, 279.0, 37.0, 2900.0, 2451.0, 2350.0, 998.0, 185.0, 1173.0, 57.0, 1378.0, 145.0, 193.0, 34.0, 630.0, 520.0, 654.0, 918.0, 652.0, 9997.0, 126.0, 3174.0, 785.0, 169.0, 336.0, 829.0, 1466.0, 26.0, 291.0, 1072.0, 81.0, 693.0, 1338.0, 530.0, 2280.0, 583.0, 138.0, 1644.0, 4498.0, 911.0, 965.0, 1313.0, 260.0, 359.0, 1111.0, 324.0, 378.0, 87.0, 815.0, 434.0, 640.0, 1522.0, 3992.0, 1436.0, 1953.0, 397.0, 2956.0, 5205.0, 1705.0, 60.0, 679.0, 2302.0, 1876.0, 5492.0, 269.0, 224.0, 43.0, 631.0, 346.0, 1781.0, 577.0, 1390.0, 1259.0, 1271.0, 2802.0, 56.0, 2231.0, 1153.0, 1966.0, 2790.0, 12.0, 555.0, 1320.0, 961.0, 299.0, 76.0, 268.0, 1041.0, 1817.0, 243.0, 298.0, 2421.0, 2427.0, 892.0, 977.0, 3217.0, 747.0, 2322.0, 315.0, 443.0, 658.0, 932.0, 3301.0, 344.0, 3.0, 50.0, 101.0, 1507.0, 1561.0, 2106.0, 798.0, 574.0, 220.0, 1640.0, 822.0, 2930.0, 897.0, 21.0, 391.0, 468.0, 5659.0, 2912.0, 561.0, 2785.0, 3758.0, 2684.0, 703.0, 1568.0, 1841.0, 3704.0, 62.0, 2133.0, 3957.0, 4942.0, 94.0, 1464.0, 1610.0, 893.0, 2934.0, 5269.0, 2206.0, 567.0, 53.0, 540.0, 1412.0, 240.0, 1917.0, 833.0, 1211.0, 1289.0, 606.0, 386.0, 4104.0, 245.0, 2397.0, 800.0, 688.0, 360.0, 1062.0, 1456.0, 406.0, 1278.0, 980.0, 832.0, 1432.0, 3713.0, 724.0, 4015.0, 358.0, 3191.0, 5.0, 4561.0, 3501.0, 1014.0, 1834.0, 2755.0, 213.0, 821.0, 1894.0, 320.0, 2883.0, 1855.0, 2712.0, 957.0, 1318.0, 969.0, 4386.0, 413.0, 817.0, 131.0, 1942.0, 723.0, 441.0, 5140.0, 2941.0, 3908.0, 1732.0, 1354.0, 636.0, 1513.0, 3032.0, 755.0, 1294.0, 8031.0, 179.0, 3124.0, 186.0, 1328.0, 48.0, 4980.0, 5423.0, 168.0, 451.0, 432.0, 2829.0, 771.0, 1964.0, 1166.0, 296.0, 4794.0, 1805.0, 7599.0, 5665.0, 257.0, 2647.0, 99.0, 8102.0, 2368.0, 116.0, 92.0, 259.0, 2415.0, 471.0, 1483.0, 3661.0, 513.0, 287.0, 538.0, 3211.0, 1545.0, 5350.0, 2876.0, 2220.0, 480.0, 8849.0, 1081.0, 12631.0, 1963.0, 6261.0, 921.0, 459.0, 1147.0, 828.0, 882.0, 2570.0, 2929.0, 497.0, 529.0, 3185.0, 3865.0, 4367.0, 1660.0, 167.0, 3169.0, 2256.0, 370.0, 5029.0, 4977.0, 7303.0, 285.0, 3007.0, 203.0, 707.0, 1829.0, 550.0, 1101.0, 1058.0, 899.0, 4288.0, 776.0, 867.0, 389.0, 1198.0, 673.0, 2308.0, 1882.0, 9862.0, 2487.0, 289.0, 3805.0, 215.0, 994.0, 2743.0, 3266.0, 1168.0, 10205.0, 3464.0, 1548.0, 2512.0, 207.0, 232.0, 2548.0, 2050.0, 1482.0, 3215.0, 271.0, 3342.0, 174.0, 5587.0, 218.0, 1845.0, 239.0, 2470.0, 3356.0, 5150.0, 3016.0, 182.0, 5050.0, 3436.0, 774.0, 139.0, 1443.0, 1565.0, 319.0, 2998.0, 165.0, 10491.0, 4308.0, 4770.0, 599.0, 720.0, 31.0, 511.0, 1736.0, 7585.0, 405.0, 1514.0, 2705.0, 2168.0, 808.0, 2349.0, 4587.0, 311.0, 5855.0, 851.0, 1097.0, 1026.0, 1549.0, 118.0, 1117.0, 23.0, 2777.0, 1940.0, 5601.0, 3288.0, 3814.0, 1184.0, 1495.0, 3205.0, 9349.0, 4876.0, 2105.0, 1152.0, 4733.0, 2557.0, 1575.0, 325.0, 6124.0, 6437.0, 1254.0, 1016.0, 3792.0, 797.0, 617.0, 4365.0, 544.0, 4161.0, 14861.0, 1822.0, 5071.0, 304.0, 1922.0, 841.0, 363.0, 3413.0, 3831.0, 600.0, 1080.0, 2885.0, 7627.0, 6007.0, 3084.0, 1191.0, 313.0, 2720.0, 3765.0, 545.0, 900.0, 1325.0, 993.0, 2643.0, 4862.0, 566.0, 2006.0, 1406.0, 3002.0, 3085.0, 2240.0, 1919.0, 1791.0, 5179.0, 4122.0, 2952.0, 7754.0, 17.0, 6001.0, 1486.0, 534.0, 1275.0, 3025.0, 2078.0, 2924.0, 1218.0, 1454.0, 478.0, 1404.0, 684.0, 2300.0, 2063.0, 1255.0, 2527.0, 3471.0, 196.0, 898.0, 148.0, 1292.0, 3354.0, 1033.0, 1679.0, 3115.0, 307.0, 859.0, 877.0, 422.0, 926.0, 4497.0, 3579.0, 8856.0, 7315.0, 10140.0, 3137.0, 172.0, 147.0, 445.0, 1701.0, 1172.0, 3060.0, 924.0, 766.0, 3574.0, 2893.0, 7261.0, 1069.0, 541.0, 738.0, 995.0, 288.0, 1189.0, 1123.0, 5378.0, 162.0, 1183.0, 4132.0, 1699.0, 810.0, 15813.0, 3064.0, 498.0, 1205.0, 30.0, 3116.0, 2969.0, 248.0, 3901.0, 84.0, 500.0, 1496.0, 744.0, 322.0, 130.0, 908.0, 3875.0, 596.0, 2910.0, 1821.0, 1673.0, 1227.0, 8070.0, 463.0, 3088.0, 4757.0, 456.0, 834.0, 11158.0, 3549.0, 4329.0, 3914.0, 435.0, 24.0, 2867.0, 15429.0, 142.0, 1020.0, 916.0, 238.0, 1672.0, 1145.0, 3393.0, 402.0, 2406.0, 1068.0, 4773.0, 5413.0, 36.0, 701.0, 1656.0, 2103.0, 519.0, 2734.0, 183.0, 338.0, 1839.0, 1155.0, 1521.0, 1943.0, 2025.0, 642.0, 535.0, 626.0, 3618.0, 937.0, 4550.0, 2408.0, 3202.0, 5011.0, 3063.0, 6884.0, 1272.0, 8240.0, 6117.0, 375.0, 1396.0, 2526.0, 8550.0, 10203.0, 4521.0, 3415.0, 8903.0, 804.0, 143.0, 11302.0, 3020.0, 255.0, 643.0, 4927.0, 4507.0, 3092.0, 302.0, 1835.0, 601.0, 1731.0, 5294.0, 2296.0, 3204.0, 3021.0, 10187.0, 1844.0, 1896.0, 205.0, 3454.0, 861.0, 1724.0, 252.0, 4647.0, 122.0, 3079.0, 981.0, 2468.0, 1897.0, 2745.0, 198.0, 1526.0, 5429.0, 779.0, 1792.0, 3179.0, 5726.0, 3383.0, 3956.0, 3165.0, 2132.0, 1028.0, 1619.0, 1050.0, 933.0, 1635.0, 3643.0, 105.0, 2203.0, 850.0, 4420.0, 2976.0, 1064.0, 4814.0, 1581.0, 5426.0, 6737.0, 1828.0, 2682.0, 157.0, 570.0, 999.0, 265.0, 2072.0, 2245.0, 502.0, 1209.0, 3508.0, 715.0, 5485.0, 7982.0, 5127.0, 10555.0, 1410.0, 1413.0, 4100.0, 4103.0, 423.0, 2228.0, 6130.0, 2843.0, 1078.0, 2588.0, 680.0, 3289.0, 3307.0, 1811.0, 1787.0, 763.0, 761.0, 10233.0, 3816.0, 6116.0, 1091.0, 2026.0, 4953.0, 2762.0, 3098.0, 4861.0, 2894.0, 5438.0, 3507.0, 2895.0, 1681.0, 18370.0, 1573.0, 258.0, 293.0, 3277.0, 528.0, 1976.0, 1445.0, 6683.0, 246.0, 549.0, 1685.0, 4364.0, 1071.0, 3168.0, 5202.0, 4946.0, 909.0, 811.0, 3589.0, 4116.0, 10179.0, 4873.0, 6908.0, 200.0, 2966.0, 1418.0, 1440.0, 1647.0, 1290.0, 1633.0, 14109.0, 3706.0, 1426.0, 6813.0, 3477.0, 1617.0, 1056.0, 415.0, 781.0, 1577.0, 862.0, 3286.0, 3194.0, 4303.0, 1019.0, 4655.0, 2124.0, 404.0, 2372.0, 5704.0, 557.0, 3433.0, 175.0, 1913.0, 1883.0, 4658.0, 2903.0, 5576.0, 4570.0, 2227.0, 5341.0, 2567.0, 2152.0, 516.0, 1528.0, 5533.0, 4999.0, 353.0, 1340.0, 5233.0, 4970.0, 584.0, 4907.0, 5936.0, 2797.0, 9984.0, 14572.0, 10757.0, 7353.0, 1728.0, 6595.0, 4926.0, 2136.0, 1945.0, 230.0, 5014.0, 6663.0, 9428.0, 1331.0, 1646.0, 8009.0, 7070.0, 210.0, 8131.0, 4541.0, 4695.0, 3322.0, 671.0, 1696.0, 2791.0, 392.0, 2157.0, 3984.0, 4537.0, 4742.0, 687.0, 2798.0, 2045.0, 329.0, 7259.0, 7727.0, 4153.0, 9267.0, 3860.0, 1108.0, 10216.0, 1491.0, 474.0, 533.0, 6096.0, 161.0, 4845.0, 1176.0, 6145.0, 476.0, 2252.0, 2062.0, 1598.0, 7306.0, 4305.0, 4931.0, 6868.0, 18988.0, 4877.0, 721.0, 4033.0, 886.0, 8876.0, 4716.0, 6026.0, 6488.0, 843.0, 1040.0, 1960.0, 2379.0, 2959.0, 2719.0, 199.0, 2311.0, 4711.0, 4624.0, 361.0, 746.0, 2075.0, 5888.0, 2224.0, 93.0, 1221.0, 704.0, 4008.0, 2251.0, 1717.0, 7370.0, 955.0, 3252.0, 1073.0, 3808.0, 610.0, 2993.0, 1809.0, 1281.0, 1652.0, 1025.0, 1572.0, 8410.0, 1580.0, 16.0, 488.0, 274.0, 9.0, 4780.0, 2892.0, 3431.0, 2277.0, 2172.0, 4276.0, 14113.0, 1367.0, 1628.0, 700.0, 395.0, 2614.0, 2657.0, 1589.0, 1733.0, 416.0, 1547.0, 3189.0, 6330.0, 14943.0, 8156.0, 11344.0, 3274.0, 2825.0, 1816.0, 10227.0, 13022.0, 7037.0, 155.0, 569.0, 1219.0, 17870.0, 1137.0, 2430.0, 3245.0, 111.0, 477.0, 4385.0, 1994.0, 2514.0, 1162.0, 1710.0, 2994.0, 2524.0, 4614.0, 665.0, 6452.0, 1704.0, 1051.0, 3180.0, 634.0, 7747.0, 3782.0, 1727.0, 8433.0, 1296.0, 2985.0, 7792.0, 1517.0, 1438.0, 3836.0, 802.0, 11316.0, 8035.0, 5034.0, 1365.0, 501.0, 2234.0, 1509.0, 1305.0, 8567.0, 2793.0, 3251.0, 13217.0, 1530.0, 7341.0, 1691.0, 4671.0, 1578.0, 1261.0, 10704.0, 706.0, 8954.0, 418.0, 901.0, 7975.0, 1541.0, 5432.0, 741.0, 737.0, 3131.0, 1450.0, 4677.0, 1655.0, 4362.0, 689.0, 3973.0, 5973.0, 2575.0, 827.0, 1063.0, 3759.0, 1779.0, 1235.0, 3852.0, 7016.0, 1849.0, 9855.0, 2275.0, 9283.0, 879.0, 89.0, 1533.0, 3212.0, 2896.0, 5812.0, 1986.0, 305.0, 10603.0, 3696.0, 4815.0, 5211.0, 14704.0, 9986.0, 6858.0, 442.0, 2764.0, 904.0, 1130.0, 3237.0, 769.0, 300.0, 941.0, 4376.0, 1415.0, 4388.0, 5636.0, 1737.0, 467.0, 5286.0, 627.0, 12347.0, 11129.0, 4632.0, 1859.0, 1750.0, 5338.0, 2279.0, 9048.0, 4803.0, 9702.0, 47.0, 2980.0, 453.0, 5825.0, 1648.0, 1693.0, 2694.0, 855.0, 2139.0, 2918.0, 80.0, 3364.0, 4371.0, 3486.0, 4805.0, 1334.0, 3450.0, 3485.0, 3640.0, 1550.0, 4040.0, 2871.0, 6489.0, 1742.0, 5489.0, 7463.0, 2701.0, 414.0, 158.0, 6504.0, 3468.0, 1082.0, 2390.0, 2610.0, 813.0, 4306.0, 5075.0, 2150.0, 4097.0, 5514.0, 339.0, 3077.0, 1036.0, 2741.0, 5674.0, 10173.0, 8315.0, 11558.0, 13845.0, 1360.0, 6034.0, 1596.0, 2964.0, 5342.0, 7509.0, 4253.0, 3439.0, 6659.0, 7516.0, 4006.0, 1700.0, 4446.0, 3164.0, 5382.0, 2858.0, 5852.0, 6241.0, 2057.0, 3099.0, 971.0, 13256.0, 1671.0, 12254.0, 1394.0, 1571.0, 7894.0, 3537.0, 943.0, 9654.0, 3854.0, 372.0, 5845.0, 7206.0, 6422.0, 3076.0, 6418.0, 5688.0, 2832.0, 1230.0, 5844.0, 4847.0, 593.0, 13392.0, 13201.0, 1980.0, 3170.0, 403.0, 7134.0, 2017.0, 3774.0, 5985.0, 2319.0, 15116.0, 3414.0, 3497.0, 4542.0, 10763.0, 16296.0, 1239.0, 2604.0, 2807.0, 3801.0, 3081.0, 3056.0, 1358.0, 1385.0, 3591.0, 4594.0, 1778.0, 2925.0, 525.0, 1134.0, 2846.0, 5530.0, 125.0, 580.0, 6754.0, 3670.0, 857.0, 1208.0, 7431.0, 8062.0, 6512.0, 3732.0, 4682.0, 151.0, 4921.0, 3411.0, 2495.0, 9644.0, 2645.0, 9078.0, 8511.0, 5687.0, 10446.0, 10479.0, 7667.0, 537.0, 1907.0, 10843.0, 3695.0, 4391.0, 940.0, 890.0, 6182.0, 4898.0, 1416.0, 2822.0, 2556.0, 6991.0, 421.0, 2021.0, 3605.0, 475.0, 11730.0, 2612.0, 9939.0, 8154.0, 3494.0, 2747.0, 136.0, 3869.0, 587.0, 8669.0, 13548.0, 946.0, 3273.0, 2313.0, 4197.0, 4480.0, 3422.0, 10766.0, 394.0, 3166.0, 3068.0, 5380.0, 2257.0, 2210.0, 1346.0, 1525.0, 3909.0, 662.0, 242.0, 1010.0, 10785.0, 297.0, 10328.0, 6154.0, 5160.0, 13521.0, 3013.0, 4567.0, 3683.0, 5197.0, 6027.0, 1314.0, 14888.0, 571.0, 367.0, 13727.0, 10260.0, 8936.0, 6153.0, 9083.0, 1350.0, 1854.0, 15798.0, 328.0, 3080.0, 14866.0, 3646.0, 1924.0, 1160.0, 2212.0, 11576.0, 2796.0, 881.0, 10820.0, 1903.0, 3161.0, 7928.0, 13940.0, 3097.0, 2792.0, 14483.0, 3554.0, 1664.0, 3880.0, 3093.0, 9259.0, 4846.0, 17234.0, 6084.0, 14308.0, 16925.0, 382.0, 276.0, 2382.0, 1102.0, 3359.0, 3676.0, 973.0, 2651.0, 4829.0, 2116.0, 4690.0, 4775.0, 5255.0, 5344.0, 2060.0, 5891.0, 6310.0, 6541.0, 2630.0, 6788.0, 2472.0, 1501.0, 6123.0, 3406.0, 8061.0, 905.0, 8161.0, 2091.0, 160.0, 3172.0, 5146.0, 8551.0, 3008.0, 8652.0, 8907.0, 1797.0, 2440.0, 3603.0, 10014.0, 2137.0, 956.0, 805.0, 3981.0, 10546.0, 10580.0, 3330.0, 1465.0, 8993.0, 1217.0, 2416.0, 3866.0, 2286.0, 639.0, 799.0, 5653.0, 3586.0, 3049.0, 11282.0, 446.0, 1088.0, 5847.0, 9365.0, 1046.0, 4409.0, 542.0, 7389.0, 3006.0, 2860.0, 651.0, 5623.0, 910.0, 6323.0, 3577.0, 3046.0, 13987.0, 13997.0, 8381.0, 3120.0, 2960.0, 3199.0, 9580.0, 2951.0, 8106.0, 10103.0, 3318.0, 3768.0, 9272.0, 1045.0, 10056.0, 7870.0, 13692.0, 4117.0, 2926.0, 2167.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ew78XHP86BYJ",
        "colab_type": "code",
        "outputId": "2e93bd1c-b993-49ba-d87f-c33293a9138a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "print(tf.random.categorical(tf.ones((1,100)),100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[41 82 85 80 13 14 74 84 89 21 52 22 89  9 76  3 75  3 16 58 95 93 93 73\n",
            "  63  6 34 78 54 21 19 82 10 67 75 52 39 28 61 46 16 27 75 37 67 33 99 21\n",
            "  45  8 91 17 75 27 98 84 46 76 33 93 21 92 25 79 45 57 65 28 33 82  7  0\n",
            "  29 85 47 56 51 38 53 91  8 59 30 68 57 75 65  9 73  5 83 64 54 43 65 73\n",
            "  49 50 63 50]], shape=(1, 100), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGC5jt7B32y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download(begin_tokens_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ-wgOIs3gkB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "begin_tokens_file_name = \"begin_tokens.\" + str(MIN_QUOTE_LENGTH) + \".\" + \\\n",
        "                         str(MAX_QUOTE_LENGTH) + \".\" + \\\n",
        "                         str(MIN_TOKEN_FREQ) + \".arr\"\n",
        "\n",
        "with open(begin_tokens_file_name, \"wb\") as arr_file:\n",
        "    pickle.dump(begin_tokens, arr_file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}